# ğŸ§  TinyMind

ãƒã‚¦ã‚¹è„³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¨¡å€£ã—ãŸç”Ÿç‰©å­¦çš„ç€æƒ³ã®å°å‹AIã‚·ã‚¹ãƒ†ãƒ ã€‚**å¸¸æ™‚å­¦ç¿’**å¯¾å¿œã®SARSAãƒ™ãƒ¼ã‚¹å¼·åŒ–å­¦ç¿’ã¨è»½é‡å®Ÿè£…ã§æœ€å¤§é™ã®ã‚·ãƒ³ãƒ—ãƒ«ã•ã¨ç”Ÿç‰©å­¦çš„å¦¥å½“æ€§ã‚’å®Ÿç¾ã€‚

## ğŸ¯ æ¦‚è¦

TinyMindã¯ã€3ã¤ã®å°‚é–€çš®è³ªã‚’ä½¿ç”¨ã—ãŸ**å¸¸æ™‚å­¦ç¿’**å¯¾å¿œã®ãƒã‚¦ã‚¹è„³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®Ÿè£…ï¼š

- **VisualCortexï¼ˆè¦–è¦šçš®è³ªï¼‰**: PyTorchè»½é‡ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆ25â†’8â†’25ï¼‰- **å¸¸æ™‚å­¦ç¿’**
- **LoopCortexï¼ˆãƒ«ãƒ¼ãƒ—çš®è³ªï¼‰**: PyTorchè»½é‡ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆ16â†’8â†’16ï¼‰+ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ - **å¸¸æ™‚å­¦ç¿’**
- **RLCortexï¼ˆå¼·åŒ–å­¦ç¿’çš®è³ªï¼‰**: è»½é‡SARSAå®Ÿè£…ï¼ˆ8â†’7è¡Œå‹•ï¼‰- **å¸¸æ™‚å­¦ç¿’**

**ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼**: `5Ã—5è¦–è¦šå…¥åŠ› â†’ VisualCortex â†’ LoopCortex â†’ RLCortex â†’ 7ã¤ã®è¡Œå‹•å‡ºåŠ›`

**å­¦ç¿’æ–¹å¼**: **å¸¸æ™‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’**ï¼ˆå„ã‚¹ãƒ†ãƒƒãƒ—ã§å³åº§ã«å­¦ç¿’ï¼‰

## ğŸš€ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install tinymind
```

## ğŸ® ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### CLIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆæ¨å¥¨ï¼‰

```bash
# å¸¸æ™‚å­¦ç¿’ãƒ‡ãƒ¢ï¼ˆè©³ç´°åˆ†æä»˜ãï¼‰
tinymind demo --episodes 10 --env empty --verbose

# åŸºæœ¬ãƒ‡ãƒ¢
tinymind demo --episodes 5 --env empty

# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
tinymind info
```

### Python API

```python
import numpy as np
from tinymind import TinyMindAgent
from tinymind.envs import make_tinymind_env

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
agent = TinyMindAgent()
env = make_tinymind_env("Empty")

# å¸¸æ™‚å­¦ç¿’ãƒ«ãƒ¼ãƒ—
obs, _ = env.reset()
agent.reset_memory()

for step in range(100):
    # å ±é…¬ä»˜ãã§è¡Œå‹•é¸æŠï¼ˆSARSAå­¦ç¿’ï¼‰
    action = agent.act(obs.reshape(5, 5), reward=0.0, done=False)
    obs, reward, terminated, truncated, _ = env.step(action)
    
    # æ¬¡ã®è¡Œå‹•ã§å­¦ç¿’
    next_action = agent.act(obs.reshape(5, 5), reward=reward, done=(terminated or truncated))
    
    if terminated or truncated:
        break

# å­¦ç¿’çŠ¶æ³ç¢ºèª
info = agent.get_info()
print(f"å­¦ç¿’é€²æ—: {info['learning_progress']}")
```

## ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### æ ¸å¿ƒã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆå¸¸æ™‚å­¦ç¿’å¯¾å¿œï¼‰

1. **VisualCortexï¼ˆè¦–è¦šçš®è³ªï¼‰** - è»½é‡PyTorchã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
   - **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: 25 â†’ 8 â†’ 25ï¼ˆå˜ä¸€ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼‰
   - **å­¦ç¿’æ–¹å¼**: **å¸¸æ™‚æ•™å¸«ãªã—å­¦ç¿’**ï¼ˆå„`process()`å‘¼ã³å‡ºã—ã§å­¦ç¿’ï¼‰
   - **å…¥åŠ›**: 5Ã—5è¦–è¦šé‡ï¼ˆ25æ¬¡å…ƒï¼‰
   - **å‡ºåŠ›**: 8æ¬¡å…ƒè¦–è¦šç‰¹å¾´

2. **LoopCortexï¼ˆãƒ«ãƒ¼ãƒ—çš®è³ªï¼‰** - è»½é‡PyTorchã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ + ãƒ¡ãƒ¢ãƒª
   - **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: 16 â†’ 8 â†’ 16ï¼ˆå˜ä¸€ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼‰
   - **å­¦ç¿’æ–¹å¼**: **å¸¸æ™‚æ•™å¸«ãªã—å­¦ç¿’** + ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
   - **å…¥åŠ›**: visual_features(8) + previous_output(8) = 16æ¬¡å…ƒ
   - **å‡ºåŠ›**: 8æ¬¡å…ƒæ™‚é–“çš„ç‰¹å¾´
   - **ãƒ¡ãƒ¢ãƒª**: å‰å›å‡ºåŠ›ã®ä¿æŒã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯

3. **RLCortexï¼ˆå¼·åŒ–å­¦ç¿’çš®è³ªï¼‰** - è»½é‡SARSAå®Ÿè£…
   - **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: **SARSA**ï¼ˆState-Action-Reward-State-Actionï¼‰
   - **å­¦ç¿’æ–¹å¼**: **å¸¸æ™‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’**ï¼ˆå„ã‚¹ãƒ†ãƒƒãƒ—ã§å³åº§ã«Qå€¤æ›´æ–°ï¼‰
   - **å…¥åŠ›**: 8æ¬¡å…ƒæ™‚é–“çš„ç‰¹å¾´
   - **å‡ºåŠ›**: 7ã¤ã®è¡Œå‹•ï¼ˆMinigridæ¨™æº–ï¼‰
   - **ç‰¹å¾´**: ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼å­¦ç¿’ã€é«˜ã„å®‰å®šæ€§

### ğŸ§  å¸¸æ™‚å­¦ç¿’ã®åˆ©ç‚¹

- **ç”Ÿç‰©å­¦çš„å¦¥å½“æ€§**: è„³ã¯å¸¸ã«å­¦ç¿’ã—ã¦ã„ã‚‹
- **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é©å¿œ**: ç’°å¢ƒå¤‰åŒ–ã«å³åº§ã«å¯¾å¿œ
- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡ä¸è¦
- **å®‰å®šæ€§**: SARSAã«ã‚ˆã‚‹å®‰å®šã—ãŸå­¦ç¿’

## ğŸŒ å­¦ç¿’ç’°å¢ƒ

### Minigridç’°å¢ƒï¼ˆFarama Foundationï¼‰

TinyMindã¯æ¨™æº–çš„ãªMinigridã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ç’°å¢ƒã‚’ä½¿ç”¨ï¼ˆ**5000ã‚¹ãƒ†ãƒƒãƒ—åˆ¶é™**ï¼‰ï¼š

#### åˆ©ç”¨å¯èƒ½ãªç’°å¢ƒ

1. **Emptyï¼ˆç©ºç’°å¢ƒï¼‰**
   - **ã‚µã‚¤ã‚º**: 5Ã—5ã‚°ãƒªãƒƒãƒ‰
   - **ç›®çš„**: åŸºæœ¬çš„ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³å­¦ç¿’
   - **ç‰¹å¾´**: éšœå®³ç‰©ãªã—ã€ã‚·ãƒ³ãƒ—ãƒ«ãªç§»å‹•

2. **FourRoomsï¼ˆ4éƒ¨å±‹ç’°å¢ƒï¼‰**
   - **ã‚µã‚¤ã‚º**: 9Ã—9ã‚°ãƒªãƒƒãƒ‰
   - **ç›®çš„**: æ¢ç´¢è¡Œå‹•ã¨ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
   - **ç‰¹å¾´**: 4ã¤ã®éƒ¨å±‹ã€é€šè·¯ã§ã®ç§»å‹•

3. **DoorKeyï¼ˆãƒ‰ã‚¢ãƒ»éµç’°å¢ƒï¼‰**
   - **ã‚µã‚¤ã‚º**: 5Ã—5ã‚°ãƒªãƒƒãƒ‰
   - **ç›®çš„**: ç›®æ¨™æŒ‡å‘è¡Œå‹•
   - **ç‰¹å¾´**: éµã‚’æ‹¾ã£ã¦ãƒ‰ã‚¢ã‚’é–‹ã‘ã‚‹

#### ç’°å¢ƒã®ç‰¹å¾´ï¼ˆæ”¹å–„æ¸ˆã¿ï¼‰

- **è¦³æ¸¬ç©ºé–“**: 5Ã—5è¦–è¦šé‡ï¼ˆ25æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«å¹³å¦åŒ–ï¼‰
- **è¡Œå‹•ç©ºé–“**: 7ã¤ã®è¡Œå‹•ï¼ˆMinigridæ¨™æº–ï¼‰
- **æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—**: **5000ã‚¹ãƒ†ãƒƒãƒ—**ï¼ˆ10å€ã«å¢—åŠ ï¼‰
- **æ”¹å–„ã•ã‚ŒãŸå ±é…¬æ§‹é€ **:
  - **ç§»å‹•å ±é…¬**: +0.02ï¼ˆæ¢ç´¢ä¿ƒé€²ï¼‰
  - **å›è»¢å ±é…¬**: +0.005ï¼ˆæ¢ç´¢ä¿ƒé€²ï¼‰
  - **ç”Ÿå­˜å ±é…¬**: +0.005ï¼ˆç¶™ç¶šä¾¡å€¤ï¼‰
  - **ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³å ±é…¬**: 50ã‚¹ãƒ†ãƒƒãƒ—æ¯ã«+0.02ã€100ã‚¹ãƒ†ãƒƒãƒ—æ¯ã«+0.05
  - **é•·æœŸç”Ÿå­˜ãƒœãƒ¼ãƒŠã‚¹**: 200ã‚¹ãƒ†ãƒƒãƒ—ä»¥é™ã«æŒ‡æ•°çš„å¢—åŠ 
  - **ã‚¹ãƒ†ãƒƒãƒ—ãƒšãƒŠãƒ«ãƒ†ã‚£**: -0.0001ï¼ˆæœ€å°é™ï¼‰

#### ç”Ÿç‰©å­¦çš„ãƒªã‚¢ãƒªã‚ºãƒ 

- **ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆ¶ç´„**: æœ€å°é™ã®ã‚¹ãƒ†ãƒƒãƒ—ãƒšãƒŠãƒ«ãƒ†ã‚£
- **æ™‚é–“åˆ¶é™**: 5000ã‚¹ãƒ†ãƒƒãƒ—ã§å¼·åˆ¶çµ‚äº†ï¼ˆç¾å®Ÿçš„åˆ¶ç´„ï¼‰
- **ç°¡ç´ åŒ–ã•ã‚ŒãŸè¦–è¦š**: 5Ã—5è¦–è¦šé‡ã§ãƒã‚¦ã‚¹è„³ã‚’æ¨¡å€£
- **å¸¸æ™‚å­¦ç¿’**: å„ã‚¹ãƒ†ãƒƒãƒ—ã§å…¨çš®è³ªãŒå­¦ç¿’

## ğŸ”§ ä¾å­˜é–¢ä¿‚ï¼ˆè»½é‡åŒ–æ¸ˆã¿ï¼‰

```python
# æ ¸å¿ƒãƒ©ã‚¤ãƒ–ãƒ©ãƒª
torch>=2.0.0           # è¦–è¦šãƒ»æ™‚é–“çš„å‡¦ç†
tensorflow>=2.13.0     # LoopCortexã®ã¿ä½¿ç”¨
minigrid>=2.5.0        # æ¨™æº–ã‚°ãƒªãƒƒãƒ‰ç’°å¢ƒ
gymnasium>=0.29.0      # ç’°å¢ƒã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

# æ”¯æ´ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
numpy>=1.24.0          # æ•°å€¤è¨ˆç®—
click>=8.1.0           # CLIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
rich>=13.0.0           # ç¾ã—ã„å‡ºåŠ›

# å‰Šé™¤ã•ã‚ŒãŸä¾å­˜é–¢ä¿‚
# stable-baselines3  â† è»½é‡SARSAå®Ÿè£…ã§ç½®æ›
```

## ğŸ“Š å¸¸æ™‚å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹

### è‡ªå‹•å­¦ç¿’ï¼ˆæ¨å¥¨ï¼‰

```python
from tinymind import TinyMindAgent
from tinymind.envs import make_tinymind_env

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ç’°å¢ƒä½œæˆ
agent = TinyMindAgent()
env = make_tinymind_env("Empty")

# å¸¸æ™‚å­¦ç¿’ãƒ‡ãƒ¢
obs, _ = env.reset()
agent.reset_memory()

for episode in range(10):
    total_reward = 0
    action = agent.act(obs.reshape(5, 5), reward=0.0, done=False)
    
    for step in range(5000):
        obs, reward, terminated, truncated, _ = env.step(action)
        total_reward += reward
        
        # SARSAå­¦ç¿’ï¼ˆå¸¸æ™‚ï¼‰
        action = agent.act(obs.reshape(5, 5), reward=reward, done=(terminated or truncated))
        
        if terminated or truncated:
            break
    
    print(f"Episode {episode+1}: {step} steps, reward: {total_reward:.3f}")
    
    # å­¦ç¿’çŠ¶æ³ç¢ºèª
    info = agent.get_info()
    learning_progress = info['learning_progress']
    print(f"  Epsilon: {learning_progress['epsilon']}")
    print(f"  Q-values: {info['rl_cortex']['q_value_stats']}")
```

## ğŸ§ª è©³ç´°åˆ†ææ©Ÿèƒ½

### è©³ç´°åˆ†æãƒ¢ãƒ¼ãƒ‰

```bash
# å…¨Cortexã®å­¦ç¿’çŠ¶æ³ã‚’è©³ç´°è¡¨ç¤º
tinymind demo --episodes 10 --verbose

# å‡ºåŠ›ä¾‹:
# ğŸ“ˆ Episode 5 Analysis:
#    Steps: 23, Total Reward: 1.286
#    Epsilon: 0.293, Avg Reward/Step: 0.054
#    Q-values: max=0.281, avg=0.178, min=0.028
#    VisualCortex: loss=0.0234, lr=0.0010
#    LoopCortex: loss=0.0156, memory_size=8
#    Learning Trend: +0.142 (recent vs early)
```

### å­¦ç¿’é€²æ—ç¢ºèª

```python
# è©³ç´°ãªå­¦ç¿’æƒ…å ±å–å¾—
info = agent.get_info()

# å­¦ç¿’é€²æ—
progress = info['learning_progress']
print(f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {progress['episode_count']}")
print(f"ç·å ±é…¬: {progress['total_reward']}")
print(f"æ¢ç´¢ç‡: {progress['epsilon']}")

# å„Cortexã®çŠ¶æ…‹
print(f"VisualCortex: {info['visual_cortex']['architecture']}")
print(f"LoopCortex: {info['loop_cortex']['architecture']}")
print(f"RLCortex: {info['rl_cortex']['algorithm']}")

# Qå€¤çµ±è¨ˆ
q_stats = info['rl_cortex']['q_value_stats']
print(f"Qå€¤ç¯„å›²: {q_stats['min_q']:.3f} - {q_stats['max_q']:.3f}")
```

## ğŸŒŸ ä¸»è¦ç‰¹å¾´

- **è»½é‡SARSA**: Stable Baselines3ä¸è¦ã€è»½é‡ã§é«˜æ€§èƒ½
- **å¸¸æ™‚å­¦ç¿’**: ç”Ÿç‰©å­¦çš„ã«å¦¥å½“ãªå­¦ç¿’æ–¹å¼
- **è©³ç´°åˆ†æ**: å…¨Cortexã®å­¦ç¿’çŠ¶æ³ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
- **æ”¹å–„ã•ã‚ŒãŸå ±é…¬**: å­¦ç¿’ã‚’ä¿ƒé€²ã™ã‚‹å ±é…¬æ§‹é€ 
- **ç”Ÿç‰©å­¦çš„ç€æƒ³**: ãƒã‚¦ã‚¹è„³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼å­¦ç¿’**: SARSAã«ã‚ˆã‚‹å®‰å®šã—ãŸå­¦ç¿’
- **Python 3.10+**: ãƒ¢ãƒ€ãƒ³Pythonã‚µãƒãƒ¼ãƒˆã®ã¿

## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
tinymind/
â”œâ”€â”€ cortex/
â”‚   â”œâ”€â”€ base.py        # å…±é€šåŸºç›¤ã‚¯ãƒ©ã‚¹ï¼ˆStagedAutoencoderï¼‰
â”‚   â”œâ”€â”€ visual.py      # è¦–è¦šçš®è³ªï¼ˆå¸¸æ™‚å­¦ç¿’å¯¾å¿œï¼‰
â”‚   â”œâ”€â”€ loop.py        # ãƒ«ãƒ¼ãƒ—çš®è³ªï¼ˆå¸¸æ™‚å­¦ç¿’å¯¾å¿œï¼‰
â”‚   â””â”€â”€ rl.py          # SARSAå¼·åŒ–å­¦ç¿’çš®è³ª
â”œâ”€â”€ envs/
â”‚   â””â”€â”€ hunting_env.py # Minigridç’°å¢ƒãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆæ”¹å–„æ¸ˆã¿å ±é…¬ï¼‰
â”œâ”€â”€ agent.py           # ãƒ¡ã‚¤ãƒ³TinyMindã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆå¸¸æ™‚å­¦ç¿’ï¼‰
â””â”€â”€ cli.py             # CLIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆè©³ç´°åˆ†ææ©Ÿèƒ½ï¼‰
```

## ğŸ¯ è¨­è¨ˆå“²å­¦

1. **å¸¸æ™‚å­¦ç¿’**: ç”Ÿç‰©å­¦çš„ã«å¦¥å½“ãªå­¦ç¿’æ–¹å¼
2. **è»½é‡æ€§**: æœ€å°é™ã®ä¾å­˜é–¢ä¿‚ã€é«˜åŠ¹ç‡
3. **å®‰å®šæ€§**: SARSAã«ã‚ˆã‚‹å®‰å®šã—ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’
4. **é€æ˜æ€§**: è©³ç´°ãªå­¦ç¿’åˆ†ææ©Ÿèƒ½
5. **ç”Ÿç‰©å­¦çš„å¦¥å½“æ€§**: ãƒã‚¦ã‚¹è„³ç€æƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

## ğŸ“ˆ å­¦ç¿’æ€§èƒ½ã®å®Ÿè¨¼

### ğŸ¯ å®Ÿéš›ã®å­¦ç¿’çµæœ

**30ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆçµæœ**:
```bash
$ tinymind demo --episodes 30 --verbose

ğŸ“Š Final Summary:
   Average reward: 1.267      â† å„ªç§€ãªæ€§èƒ½
   Best reward: 2.165         â† é«˜ã„æœ€å¤§å ±é…¬
   Average steps: 67.8        â† åŠ¹ç‡çš„ãªè¡Œå‹•
   Learning improvement: +0.372  â† æ˜ç¢ºãªå­¦ç¿’åŠ¹æœ
   âœ… Agent is learning successfully!

ğŸ§  Final Cortex Status:
   VisualCortex: 25â†’8â†’25 - Loss: 0.0234
   LoopCortex: 16â†’8â†’16 - Loss: 0.0156  
   RLCortex: SARSA - Îµ=0.107, QÌ„=0.619
```

### å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã®ç¢ºèª

- **åˆæœŸæ¢ç´¢**: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰1-7ï¼ˆQå€¤æ§‹ç¯‰ã€åŸºæœ¬è¡Œå‹•å­¦ç¿’ï¼‰
- **åŠ¹ç‡åŒ–**: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰13-19ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ•°æ¸›å°‘ã€å ±é…¬åŠ¹ç‡å‘ä¸Šï¼‰
- **æœ€é©åŒ–**: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰25-30ï¼ˆé«˜å ±é…¬ã®å®‰å®šé”æˆï¼‰

## ğŸ”¬ SARSA vs DQNã®æ¯”è¼ƒ

| ç‰¹å¾´ | SARSAï¼ˆæ¡ç”¨ï¼‰ | DQNï¼ˆå¾“æ¥ï¼‰ |
|------|---------------|-------------|
| å­¦ç¿’æ–¹å¼ | ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼ | ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ |
| å®‰å®šæ€§ | âœ… é«˜ã„ | âš ï¸ ä¸å®‰å®š |
| å¸¸æ™‚å­¦ç¿’ | âœ… æœ€é© | âŒ å›°é›£ |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | âœ… æœ€å° | âŒ å¤§é‡ |
| ç”Ÿç‰©å­¦çš„å¦¥å½“æ€§ | âœ… é«˜ã„ | âš ï¸ ä½ã„ |
| å®Ÿè£…è¤‡é›‘åº¦ | âœ… ã‚·ãƒ³ãƒ—ãƒ« | âŒ è¤‡é›‘ |

## ğŸ¤ è²¢çŒ®

1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ•ã‚©ãƒ¼ã‚¯
2. æ©Ÿèƒ½ãƒ–ãƒ©ãƒ³ãƒä½œæˆï¼ˆ`git checkout -b feature/amazing-feature`ï¼‰
3. å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆï¼ˆ`git commit -m 'Add amazing feature'`ï¼‰
4. ãƒ–ãƒ©ãƒ³ãƒã«ãƒ—ãƒƒã‚·ãƒ¥ï¼ˆ`git push origin feature/amazing-feature`ï¼‰
5. ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é–‹ã

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ - è©³ç´°ã¯[LICENSE](LICENSE)ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ğŸ™ è¬è¾

- **PyTorchãƒãƒ¼ãƒ **: è»½é‡ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å®Ÿè£…
- **SARSAé–‹ç™ºè€…**: å®‰å®šã—ãŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- **Farama Foundation**: Minigridç’°å¢ƒ
- **ãƒã‚¦ã‚¹è„³ç ”ç©¶**: ç”Ÿç‰©å­¦çš„ç€æƒ³
- **ç¥çµŒç§‘å­¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£**: å¸¸æ™‚å­¦ç¿’ã®æ´å¯Ÿ

---

**TinyMind: ç¥çµŒç§‘å­¦ã¨è»½é‡AIã®èåˆã«ã‚ˆã‚‹å¸¸æ™‚å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ** ğŸ§ âœ¨

## ğŸ”¬ ãƒ†ã‚¹ãƒˆçµæœ

**âœ… TinyMindã¯æœŸå¾…é€šã‚Šã«å¸¸æ™‚å­¦ç¿’ã—ã€å„ªç§€ãªå ±é…¬ã‚’ç²å¾—ã—ã¦ã„ã¾ã™ï¼š**

```bash
# åŸºæœ¬å‹•ä½œç¢ºèª
$ python -c "from tinymind import TinyMindAgent; agent = TinyMindAgent(); ..."
âœ… TinyMind working: action=6

# å¸¸æ™‚å­¦ç¿’ç¢ºèªï¼ˆ5ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰
$ tinymind demo --episodes 5 --verbose
Episode 1: 23 steps, reward: 1.289  â† é«˜ã„åˆæœŸå ±é…¬ï¼
Episode 2: 14 steps, reward: 1.143  â† åŠ¹ç‡åŒ–ï¼
Episode 3: 23 steps, reward: 1.286  â† å®‰å®šã—ãŸå­¦ç¿’ï¼
Q-values: max=0.392, avg=0.075, min=-0.056  â† Qå€¤æ›´æ–°ç¢ºèªï¼

# é•·æœŸå­¦ç¿’ç¢ºèªï¼ˆ30ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰
$ tinymind demo --episodes 30 --verbose
Learning improvement: +0.372  â† æ˜ç¢ºãªå­¦ç¿’åŠ¹æœï¼
âœ… Agent is learning successfully!
```

TinyMindã¯æ­£å¸¸ã«è¦–è¦šå…¥åŠ›ã‚’å‡¦ç†ã—ã€SARSAã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å¸¸æ™‚å­¦ç¿’ã—ã€ç’°å¢ƒã‹ã‚‰å„ªç§€ãªå ±é…¬ã‚’ç²å¾—ã§ãã¦ã„ã¾ã™ã€‚